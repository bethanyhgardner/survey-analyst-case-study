---
title: "Survey Data Analysis Case Study"
author: "Bethany Gardner"
date: 08/29/2024
format:
  revealjs:
    embed-resources: true
    footer: github.com/bethanyhgardner/survey-analyst-case-study
    incremental: true
    slide-number: true
    smaller: true
    theme: white
bibliography: references.bib
---

```{r}
#| label: setup

library(tidyverse)  # general data wrangling and plots
library(igraph)  # calculate flowchart positions
library(kableExtra)  # tables

theme_blue <- "#5692e4"  # revealjs theme blue
```

## Why this project?

-   Using one of my dissertation experiments as a case study (published as [GitHub Repository](https://github.com/bethanyhgardner/dissertation/) and [Quarto Book](https://bethanyhgardner.github.io/dissertation/))
-   I've chosen to talk about this one because it involves the most data preprocessing/management steps, and I was in charge of all of them

## About the experiment

-   Studying language learning and processing mechanisms for singular *they*
-   Including pronouns on nametags and in introductions are common recommendations for creating a more gender-inclusive environment. We know it can affect people's perception of
an environment, but does it also affect people's language *use*?
-   Participants:
    -   Learned about a set of fictional **characters** (he/him, she/her, and they/them)
    -   **Nametag condition:** Varied whether the introductions to the character explicitly stated their pronouns (*This is Alex, who uses they/them pronouns. They...*)
    -   **Introduction condition:** Varied whether the nametags included pronouns
    -   **Speech production task** eliciting possessive pronouns (*Alex gave the apple to their brother.*)
    -   **Survey** about their demographics, experience with singular *they*, and attitudes about singular *they*

## About the data

-   Audio data transcribed and annotated for which pronouns were produced & survey data for each participant
-   Do the nametag and introduction conditions affect accuracy producing singular *they*?
-   If production accuracy is internally reliable, is it predicted by demographics, language attitude, or language experience measures?

## Pipeline overview

```{r}
#| label: pipeline-data

# calculate data structure for flowchart using this tutorial:
# https://www.r-bloggers.com/2022/06/creating-flowcharts-with-ggplot2/
pipeline_start <- tribble(
  ~from, ~to,
  "Designing survey questions & speech task", "Power analysis (R)",
  "Power analysis (R)", "Text data (PCIbex)",
  "Power analysis (R)", "Audio data (AWS S3)",
  "Text data (PCIbex)", "Preprocess (R)",
  "Audio data (AWS S3)", "Transcribe using whisper (Python)",
  "Transcribe using whisper (Python)", "Check transcriptions",
  "Preprocess (R)", "Merge data (R)",
  "Check transcriptions", "Merge data (R)",
  "Merge data (R)", "Check internal reliability (R)",
  "Check internal reliability (R)", "Multilevel model (R)"
)

pipeline_graph <- graph_from_data_frame(pipeline_start, directed = TRUE)
pipeline_tree <-  layout_as_tree(pipeline_graph)

pipeline_nodes <- pipeline_tree |>
  as_tibble(.name_repair = "unique") |>
  rename(y = 1, x = 2) |>
  mutate(
    label = vertex_attr(pipeline_graph, "name"),
    x = x * -1,
    xmin = x - 0.4,
    xmax = x + 0.4,
    ymin = y - 0.2,
    ymax = y + 0.2
  ) |>
  relocate(c(label, x, y), .before = 0) |>
  mutate(across(
    starts_with("y"),
    \(x) ifelse(
      str_detect(label, "(Merge)|(internal)|(Multilevel)"),
      x + 0.5, x
    )
  )) |>
  mutate(across(
    starts_with("x"),
    \(x) case_when(
      str_detect(label, "(Merge)|(internal)|(Multilevel)") ~ x + 1,
      str_detect(label, "(Text)|(Preprocess)") ~ x + 0.5,
      .default = x
    )
  ))

pipeline_edges <- pipeline_start |>
  mutate(id = row_number()) |>
  pivot_longer(cols = c("from", "to"), names_to = "s_e", values_to = "label") |>
  left_join(pipeline_nodes, by = "label") |>
  mutate(x = ifelse(s_e == "from", xmax, xmin)) |>
  select(id, s_e, x, y)
```

```{r}
#| label: pipeline-plot-1
#| fig-width: 10

ggplot() +
  geom_rect(
    data = pipeline_nodes,
    aes(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax),
    fill = theme_blue
  ) +
  geom_text(
    data = pipeline_nodes,
    aes(x = x, y = y, label = str_wrap(label, 10)),
    size = 4.25
  ) +
  geom_path(
    data = pipeline_edges,
    aes(x = x, y = y, group = id),
    linewidth = 1,
    arrow = arrow(length = unit(0.02, "snpc"), type = "closed")
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void()
```

## Power analysis

Create a data structure with the structure of the proposed experiment, and estimate fixed and random effect sizes based on prior experiments.

```{r}
#| label: power-data-structure
#| eval: false
#| echo: true

# get Pronoun * PSA interaction from Exp2 production model
load("r_data/exp2.RData")

exp2_r_effect_size <- exp2_m_prod@model |>
  tidy() |>
  filter(term == "Pronoun=They_HeShe:PSA=GenLang") |>
  pull(estimate) |>
  round(2)

exp2_r_effect_size       # log-odds
exp(exp2_r_effect_size)  # odds ratio

# start with 108 participants each doing 30 trials
exp3_pw_data_struct <- data.frame(
  Participant = rep(as.factor(1:108), each = 30),
  Trial = rep(as.factor(1:30), 108)
)

# Trials are split between 3 Pronoun Pair conditions, which are contrast-coded
# to compare:
# (1) They|HeShe vs HeShe|They + HeShe|SheHe
# (2) HeShe|They vs HeShe\|SheHe
exp3_pw_data_struct <- exp3_pw_data_struct |>
  bind_cols(
    "Pronoun" = rep(rep(factor(c("He", "She", "They")), each = 10), 108)
  )
contrasts(exp3_pw_data_struct$Pronoun) <- cbind(
  "_T vs HS" = c(.33, .33, -.66),
  "_H vs S"  = c(-.5, .5, 0)
)

# Nametag and Introduction conditions vary in a 2x2 between-P design, and both
# are mean-centered effects coded.
exp3_pw_data_struct <- exp3_pw_data_struct |>
  bind_cols(
    "Nametag" = rep(rep(factor(c(0, 0, 1, 1)), each = 30), 108 / 4),
    "Intro" = rep(rep(factor(c(0, 1, 0, 1)), each = 30), 108 / 4)
  )

contrasts(exp3_pw_data_struct$Nametag) <- cbind("_No_Yes" = c(-.5, .5))
contrasts(exp3_pw_data_struct$Intro) <- cbind("_No_Yes" = c(-.5, .5))

# Item is defined as each unique image-name-pronoun combination. There are 6
# sets of characters, and each list sees 3, making 18 unique characters.
exp3_pw_data_struct <- exp3_pw_data_struct |>
  bind_cols(
    "Character" = rep(as.factor(1:18), each = 30 / 3, 108 / 6)
  )
str(exp3_pw_data_struct)

exp3_pw_data_struct |>
  group_by(Nametag, Intro) |>
  summarise(n_distinct(Participant))

# The closest thing to existing data is the Exp2 (written) production task.
# Since interpreting effect sizes is apparently more complicated for logistic
# regression, let's go with the Exp2 results as a baseline. That's a rough
# estimate of how much harder they/them is to produce than he/him and she/her.
# And let's set the hypothetical Nametag and Introduction effects to be about
# the same size as the PSA. Hopefully that's small enough to be kind of
# conservative with the power analysis, but not aiming for effects too small to
# be practically relevant.
exp2_m_prod_fixed <- exp2_m_prod@model |>
  tidy() |>
  filter(effect == "fixed") |>
  select(term, estimate)
exp2_m_prod_fixed

# Predictions for Exp3 based on ranges from Exp2:
exp3_pw_fixed <- c(
  +0.75,  # Intercept                    Medium
  +3.00,  # Pronoun: T vs HS             Largest
  -0.10,  # Pronoun: H vs S              NS, maybe small
  +0.10,  # Nametag                      NS, maybe small
  +0.10,  # Introduction                 NS, maybe small
  -2.00,  # Pronoun: T vs HS * Nametag   Same size as PSA interaction
  -0.10,  # Pronoun: H vs S  * Nametag   NS, maybe small
  -2.00,  # Pronoun: T vs HS * Intro     Same size as PSA interaction
  -0.10,  # Pronoun: H vs S  * Intro     NS, maybe small
  +0.25,  # Nametag * Intro              Maybe small
  -2.00,  # 3 way T vs HS                Same size as PSA interaction
  -0.10   # 3 way H vs S                 NS, maybe small
)

# The model for the Exp2 production task only converged with random intercepts
# by item, and no random effects by participant.
exp2_m_prod_random <- VarCorr(exp2_m_prod@model)

# The model for the Exp1 production task only converged with random intercepts
# and slopes by participant, and no random effects by item.
load("r_data/exp1.RData")
exp1_m_prod_random <- VarCorr(exp1a_m_prod@model)


# So, I'll combine those two as a starting place to estimate the random effects.
# It's possible the actual data won't converge with the maximal random effects
# structure, but for now let's assume it will.
exp3_pw_random <- exp1_m_prod_random
exp3_pw_random[["Item"]] <- exp2_m_prod_random[["Name"]]

# Create model with this data structure, fixed effects, and random effects
exp3_pw_m_108 <- makeGlmer(
  formula = SimAcc ~ Pronoun * Nametag * Intro +
    (Pronoun | Participant) + (1 | Character),
  family = binomial,
  fixef = exp3_pw_fixed,
  VarCorr = exp3_pw_random,
  data = exp3_pw_data_struct
)
summary(exp3_pw_m_108)
```

## Power analysis

Use {simr} [@green2016] to simulate the power for each effect (Pronoun &times; Nametag/Intro, Pronoun &times; Nametag &times; Intro) at 108, 132, 156, and 180 participants.

```{r}
#| label: power-data-simulation
#| eval: false
#| echo: true

# Simulate data
exp3_pw_sim_data <- doSim(exp3_pw_m_108)
exp3_pw_data_struct <- exp3_pw_data_struct |>
  bind_cols("SimAcc" = exp3_pw_sim_data)

summary(exp3_pw_data_struct)

# Code to run simulation:
powerSim(
  exp3_pw_m_108,
  nsim = 1000,
  test = fixed("Pronoun_T vs HS:Nametag_No_Yes", "z")
)

# Then extend model to larger N
exp3_pw_m_132 <- extend(exp3_pw_m_108, along = "Participant", n = 132)

# Load and join results
exp3_pw_results <- bind_rows(
    .id = "sim",
    "2_108" = readRDS("r_data/exp3_power_2way_N108.RDA") |> summary(),
    "2_132" = readRDS("r_data/exp3_power_2way_N132.RDA") |> summary(),
    "2_156" = readRDS("r_data/exp3_power_2way_N156.RDA") |> summary(),
    "2_180" = readRDS("r_data/exp3_power_2way_N180.RDA") |> summary(),
    "3_132" = readRDS("r_data/exp3_power_3way_N132.RDA") |> summary(),
    "3_156" = readRDS("r_data/exp3_power_3way_N156.RDA") |> summary()
  ) |>
  mutate(
    n_participants = str_sub(sim, 3),
    effect = case_when(
      str_sub(sim, 0, 1) == "2" ~ "Pronoun * Nametag/Intro",
      str_sub(sim, 0, 1) == "3" ~ "Pronoun * Nametag * Intro"
    )
  ) |>
  column_to_rownames(var = "sim")
```

## Power analysis 

-   We determined that 156 participants, each completing 30 trials, would have 0.93 \[0.91, 0.94\] power at α = .05 to detect the two-way interactions (Pronoun × Nametag/ Introduction).
-   Note that in cognitive psychology, the goal is have enough statistical power to detect differences between experimental conditions, not necessarily to be able to generalize differences between groups of participants to the entire population.
-   We can get a decently representative sample of respondents from Prolific, but didn't do population weights.

## Pipeline overview

```{r}
#| label: pipeline-plot-2
#| fig-width: 10

# highlight first section
ggplot() +
  geom_rect(
    data = pipeline_nodes |>
      mutate(group = case_when(
        str_starts(label, "Audio") ~ "highlight",
        str_starts(label, "Transcribe") ~ "highlight",
        str_starts(label, "Check t") ~ "highlight",
        .default = "fade"
      )),
    aes(xmin = xmin, ymin = ymin, xmax = xmax, ymax = ymax, fill = group)
  ) +
  geom_text(
    data = pipeline_nodes,
    aes(x = x, y = y, label = str_wrap(label, 10)),
    size = 4.25
  ) +
  geom_path(
    data = pipeline_edges,
    aes(x = x, y = y, group = id),
    linewidth = 1,
    arrow = arrow(length = unit(0.02, "snpc"), type = "closed")
  ) +
  scale_fill_manual(values = c("grey75", theme_blue)) +
  scale_x_continuous(expand = c(0, 0)) +
  theme_void() +
  guides(fill = guide_none())
```

## Audio data (AWS S3)

-   PCIbex, our experiment platform, sends the audio data to an AWS S3 bucket
-   It's most efficient to just download the data from S3 once and run the rest of the analyses locally, instead of querying it from S3 every time
-   Bash script to download new data; check that an audio file for each trial for each participant exists as expected; then unzip, convert, and sort audio files

## Audio data (AWS S3)

```{bash}
#| label: bash-script
#| eval: false
#| echo: true

# Options:
#   s   sync data from AWS
#   p   check participant list
#   z   unzip and sort audio files
#   c   run tests on PCIbex output and audio file names
#   t   transcribe


while getopts "spzct" option; do
  case $option in
    s)  # Sync audio data from S3
        echo "Getting data from AWS"
        cd ../data/s3/
        aws s3 sync s3://they3 .
        cd ../../preprocessing/
        ;;
    p)  # Get list of participants from PCIbex data to update participant list
        echo "Checking audio data to see what needs to be added to the participant list"
        Rscript participant_list.R
        ;;
    z)  # Unzip the audio data and convert it to WAV files in dirs for each participant
        echo "Unzipping, converting, and sorting the audio files"
        python s3_to_wav.py
        ;;
    c)  # Check output
        echo "Checking the audio file names against the PCIbex data"
        Rscript check_output.R
        ;;
    t)  # Transcribe
        echo "Transcribing"
        python transcribe.py
        ;;
  esac
done
```

## Transcribe using *whisper*

-   First pass for transcription using the *whisper* model [@radford]
-   Pros: fairly quick, runs locally and does not get copy of identifiable data
-   Cons: does not include speech errors and disfluencies

## Transcribe using *whisper*

```{python}
#| label: whisper
#| python.reticulate: false
#| eval: false
#| echo: true

import os
import whisper
import pandas as pd
from pathlib import Path


# ---- Helper functions ----- #
def make_transcription_df():
    """Set up df for transcription data.

    Returns:
        df: columns for `participant_id`, `prolific_id`, `trial_id`, indexed by
            `file_path`
    """    
    transcriptions = []
    participant_dirs = [
        p for p in audio_dir.iterdir()
        if not p.match("*temp*") and not p.match("*incomplete*")
    ]

    for p in participant_dirs:
        audio_list = [a.stem for a in p.glob('*.wav')]
        trials = [get_trial_info(p.name, a) for a in audio_list]
        df = pd.DataFrame(
            trials,
            columns=['file_path', 'participant_id', 'prolific_id', 'trial_id'],
        )
        df = df.set_index('file_path').sort_values(by='trial_id')
        transcriptions.append(df)
    return transcriptions


def get_trial_info(p_dir, file_name):
    """Get trial info from the name of the audio file.

    Args:
        p_dir (str): dir for participant data
        file_name (str): audio file within participant's data dir 

    Returns:
        list: .wav file name (Path), participant ID (str), prolific ID (str),
            and trial ID (str)
    """    
    participant_id, prolific_id = p_dir.split('_')
    trial_id = file_name.removeprefix(prolific_id + '_').removesuffix('.wav')
    return [
        audio_dir / p_dir / f"{file_name}.wav",
        participant_id, prolific_id, trial_id
    ]


def run_whisper_on_participant(df, model):
    """Use whisper to transcribe a trial.

    Args:
        df (df): structure for transcription data from `make_transcription_df()`,
            which has `participant_id` as the first column and is indexed by the
            path to the audio file
        model (whisper model): whisper model loaded (using small English-only)
    """    
    participant_id = df.iloc[0, 0]
    file_path = text_dir / f"{participant_id}_whisper.csv"
    if not os.path.exists(file_path):
        print(participant_id)
        df['text'] = df.index.map(lambda t: whisper.transcribe(model, str(t))['text'])
        print(df['text'])
        df.to_csv(os.path.join(file_path))
     

# ---- Main function ----- #
def transcribe_trials():
    """Main function to transcribe .wav files using whisper."""    
    transcriptions = make_transcription_df()
    model = whisper.load_model('medium.en')
    for p in transcriptions:
        run_whisper_on_participant(p, model)
        
    return transcriptions


# ---- Run ----- #
audio_dir = Path('..') / 'data' / 'exp2_audio'
text_dir = Path('..') / 'data' / 'exp2_transcription'

transcribe_trials()
```

## Check transcriptions {.scrollable}

-   RA listened to audio and added back in disfluencies
-   Coded for which pronouns are produced; accuracy determined by final pronoun

```{r}
#| label: example-data

read.csv("example_data.csv") |>
  rename_with(~str_replace_all(.x, "_", " ")) |>
  kbl() |>
  kable_styling()
```

## Check transcriptions

-   Tests to check coding against regex and for completion

```{r}
#| label: check-data
#| eval: false
#| echo: true

library(here)
library(tidyverse)
library(testthat)
library(readxl)


# Load data from coded CSVs----
df <- list.files(
  path = "data/exp2_coding",
  pattern = "*coded.csv",
  full.names = TRUE
) |>
  set_names() |>
  map(read.csv) |>
  list_rbind(names_to = "participant_id") |>
  mutate(participant_id = str_sub(str_split_i(participant_id, "/", 3), 0, 4)) |>
  mutate(
    .before = trial_id,
    character_list = str_remove(str_split_i(trial_id, "_", 2), "list")
  ) |>
  mutate(across(
    c(ends_with("id"), contains("pronoun"), condition, character_list),
    as.factor
  )) |>
  mutate(across(
    starts_with("transcription"),
    \(x) {
      case_when(
        x == "" ~ NA,
        x == " ." ~ NA,
        .default = str_replace(str_trim(x), "Jamie", "Jaime")  # fix spelling
      )
    }
  )) |>
  mutate(drop_trial = case_when(  # don't drop trial if sentence is cut off
    str_detect(transcription_manual, "Alex") ~ 0,
    str_detect(transcription_manual, "Casey") ~ 0,
    str_detect(transcription_manual, "Jaime") ~ 0,
    str_detect(transcription_manual, "Jordan") ~ 0,
    str_detect(transcription_manual, "Sam") ~ 0,
    str_detect(transcription_manual, "Taylor") ~ 0,
    .default = drop_trial
  )) |>
  filter(str_starts(participant_id, "P"))  # drop incompletes


# Calculate pronoun produced and accuracy----
df2 <- df |>
  mutate(
    sum = his + her + their,
    multiple_pronouns = ifelse(sum > 1, 1, 0),
    he_loc = ifelse(
      multiple_pronouns == 1 & he == 1,
      str_locate(str_to_lower(transcription_manual), "\\bhe"), 0
    ),
    his_loc = ifelse(
      multiple_pronouns == 1 & his == 1,
      str_locate(str_to_lower(transcription_manual), "\\bhi"), 0
    ),
    she_loc = ifelse(
      multiple_pronouns == 1 & she == 1,
      str_locate(str_to_lower(transcription_manual), "\\bshe\\b"), 0
    ),
    her_loc = ifelse(
      multiple_pronouns == 1 & her == 1,
      str_locate(str_to_lower(transcription_manual), "\\bher\\b"), 0
    ),
    they_loc = ifelse(
      multiple_pronouns == 1 & they == 1,
      str_locate(str_to_lower(transcription_manual), "\\bthey\\b"),
      0
    ),
    their_loc = ifelse(
      multiple_pronouns == 1 & their == 1,
      str_locate(str_to_lower(transcription_manual), "\\bthei"),
      0
    ),
    max_loc = ifelse(
      multiple_pronouns == 1,
      pmax(he_loc, his_loc, she_loc, her_loc, they_loc, their_loc),
      NA
    ),
    pronoun_produced = factor(
      case_when(
        sum == 0 ~ "none",
        multiple_pronouns == 0 & (his == 1 | he == 1) ~ "his",
        multiple_pronouns == 0 & (her == 1 | she == 1) ~ "her",
        multiple_pronouns == 0 & (their == 1 | they == 1) ~ "their",
        multiple_pronouns == 1 & max_loc == he_loc ~ "his",
        multiple_pronouns == 1 & max_loc == his_loc ~ "his",
        multiple_pronouns == 1 & max_loc == she_loc ~ "her",
        multiple_pronouns == 1 & max_loc == her_loc ~ "her",
        multiple_pronouns == 1 & max_loc == they_loc ~ "their",
        multiple_pronouns == 1 & max_loc == their_loc ~ "their",
        is.na(max_loc) & participant_id == "P329" &
          str_detect(transcription_manual, "chocolate") ~ "his",
        is.na(max_loc) & participant_id == "P492" &
          str_detect(transcription_manual, "pencil") ~ "their"
      ),
      levels = c("his", "her", "their", "none")
    ),
    accuracy = case_when(
      pronoun_produced == "none" ~ NA,
      as.character(pronoun_produced) == "his" &
        as.character(target_pronoun) == "he" ~ 1,
      as.character(pronoun_produced) == "her" &
        as.character(target_pronoun) == "she" ~ 1,
      as.character(pronoun_produced) == "their" &
        as.character(target_pronoun) == "they" ~ 1,
      .default = 0
    )
  )

# Drop trials with no data, drop calculation/extra columns, add item ID----
df3 <- df2 |>
  filter(drop_trial == 0 & !is.na(transcription_manual)) |>
  rename(transcription = transcription_manual) |>
  select(
    -prolific_id, -correct_description, -transcription_whisper, -name_only,
    -drop_trial, -notes, -sum, -ends_with("loc")
  )

character_lists <- read_excel(
  path = here("materials", "exp2_stimuli.xlsx"),
  sheet = "Character Sets",
  skip = 2,
  col_types = c("skip", rep("text", 7), rep("skip", 9))
) |>
  rename_with(str_to_lower) |>
  mutate(across(everything(), as.factor))

df3 <- df3 |>
  left_join(
    character_lists |> select(list, item, pronouns),
    by = join_by(character_list == list, target_pronoun == pronouns),
    relationship = "many-to-one"
  ) |>
  relocate(item, .after = target_pronoun) |>
  rename(target_id = item) |>
  select(-character_list)


# Checks----
test_that("No NAs in pronoun variables", {
  expect_false(any(is.na(df3$he)))
  expect_false(any(is.na(df3$his)))
  expect_false(any(is.na(df3$she)))
  expect_false(any(is.na(df3$her)))
  expect_false(any(is.na(df3$they)))
  expect_false(any(is.na(df3$their)))
  expect_false(any(is.na(df3$name_only)))
})

test_that("Pronoun variables match regex", {
  df_test <- df3 |>
    mutate(transcription = str_to_lower(transcription)) |>
    select(participant_id, transcription, he, his, she, her, they, their) |>
    mutate(
      has_he = case_when(
        str_detect(transcription, "\\bhe\\b") ~ 1,
        .default = 0
      ),
      has_his = case_when(
        str_detect(transcription, "\\bhis\\b") ~ 1,
        str_detect(transcription, "\\bhi-") ~ 1,
        .default = 0
      ),
      has_she = case_when(
        str_detect(transcription, "\\bshe\\b") ~ 1,
        .default = 0
      ),
      has_her = case_when(
        str_detect(transcription, "gave her a") ~ 0,
        str_detect(transcription, "her glasses back") ~ 0,
        str_detect(transcription, "pencil to h--") ~ 1,
        str_detect(transcription, "\\bher\\b") ~ 1,
        .default = 0
      ),
      has_they = case_when(
        str_detect(transcription, "\\bthey\\b") ~ 1,
        .default = 0
      ),
      has_their = case_when(
        str_detect(transcription, "\\btheir\\b") ~ 1,
        str_detect(transcription, "to thei-") ~ 1,
        str_detect(transcription, "to theirs") ~ 1,
        str_detect(transcription, "chocolate to th--") ~ 1,
        .default = 0
      )
    )

  expect_true(all(df_test$he == df_test$has_he))
  expect_true(all(df_test$his == df_test$has_his))
  expect_true(all(df_test$she == df_test$has_she))
  expect_true(all(df_test$her == df_test$has_her))
  expect_true(all(df_test$they == df_test$has_they))
  expect_true(all(df_test$their == df_test$has_their))
})

test_that("All multiple pronoun trials coded as disfluencies and final coded", {
  df_dis <- df3 |>
    filter(multiple_pronouns == 1 & disfluency == 0) |>
    filter(!(he == 1 & his == 1)) |>
    filter(!(she == 1 & her == 1)) |>
    filter(!(they == 1 & their == 1))
  expect_equal(nrow(df_dis), 0)

  expect_false(any(is.na(df3$pronoun_produced)))
})


# Counts----
participants_missing_trials <- df |>
  filter(!is.na(transcription_manual) & drop_trial == 0) |>
  summarise(.by = participant_id, n = n_distinct(trial_id))
sum(participants_missing_trials$n)

df3 |>
  summarise(
    .by = c(condition, target_pronoun),
    mean = mean(accuracy, na.rm = TRUE)
  ) |>
  arrange(target_pronoun)


# Export----
write_csv(df3, file = "data/exp2_pronouns.csv")
```

## References
